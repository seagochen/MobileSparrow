import torch
from copy import deepcopy
from tqdm import tqdm


# =========================
# 1) EMA: æŒ‡æ•°ç§»åŠ¨å¹³å‡
# =========================
class EMA:
    """
    æ¨¡å‹å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆExponential Moving Averageï¼‰ã€‚

    ä½œç”¨
    ----
    - ç»´æŠ¤ä¸€ä¸ªâ€œå½±å­æ¨¡å‹â€ï¼ˆema_modelï¼‰ï¼Œå…¶æƒé‡æ˜¯åœ¨çº¿æ¨¡å‹å‚æ•°çš„å¹³æ»‘å¹³å‡ã€‚
    - åœ¨éªŒè¯/å¯¼å‡ºæ—¶ä½¿ç”¨ EMA æƒé‡ï¼Œé€šå¸¸æ›´ç¨³å®šï¼Œç²¾åº¦ç•¥ä¼˜ã€‚

    ç”¨æ³•
    ----
    >>> ema = EMA(model, decay=0.9999)
    >>> for each training step:
    ...     optimizer.step()
    ...     ema.update(model)  # è®­ç»ƒåæ›´æ–° EMA
    >>> evaluate(ema.ema_model, ...)  # ä½¿ç”¨ ema_model éªŒè¯/ä¿å­˜

    å‚æ•°
    ----
    model : nn.Module
        éœ€è¦è¢«è·Ÿè¸ªçš„åœ¨çº¿æ¨¡å‹ï¼ˆä¼š deepcopy ä¸€ä»½ï¼‰ã€‚
    decay : float
        EMA è¡°å‡ç³»æ•°ï¼Œè¶Šæ¥è¿‘ 1 è¶Šâ€œå¹³æ»‘â€ã€‚æ­¤å¤„è¿˜å åŠ äº†ä¸€ä¸ªéšæ›´æ–°æ­¥æ•°å˜åŒ–çš„ warmup å› å­ã€‚

    å¤‡æ³¨
    ----
    - ä»…å¯¹ requires_grad çš„å‚æ•°åš EMAï¼›
    - ema_model ç½®ä¸º eval() å¹¶å†»ç»“æ¢¯åº¦ï¼ˆä¸å‚ä¸åä¼ ï¼‰ã€‚
    """

    def __init__(self, model, decay=0.9999):
        self.ema_model = deepcopy(model).eval()  # æ¨¡å‹æ·±æ‹·è´ï¼ˆä¸å…±äº«å‚æ•°ï¼‰
        self.decay = decay
        self.updates = 0  # å·²æ›´æ–°çš„æ­¥æ•°ç»Ÿè®¡

        # EMA æ¨¡å‹ä¸éœ€è¦æ¢¯åº¦
        for p in self.ema_model.parameters():
            p.requires_grad_(False)

    def update(self, model):
        """
        ç”¨åœ¨çº¿æ¨¡å‹çš„å½“å‰å‚æ•°æ›´æ–° EMA æ¨¡å‹ã€‚

        ç­–ç•¥
        ----
        - ä½¿ç”¨åŠ¨æ€è¡°å‡ï¼šd = decay * (1 - 0.9 ** (updates / 2000))
          å‰æœŸæ›´â€œè·Ÿéšâ€ï¼ŒåæœŸæ›´â€œå¹³æ»‘â€ã€‚

        æ³¨æ„
        ----
        - ä»…å¯¹ requires_grad çš„å‚æ•°æ‰§è¡Œ EMAï¼›
        - å‡è®¾ä¸¤è¾¹çš„ named_parameters èƒ½ä¸€ä¸€å¯¹åº”ï¼ˆåŒååŒç»“æ„ï¼‰ã€‚
        """
        self.updates += 1
        d = self.decay * (1 - pow(0.9, self.updates / 2000))  # åŠ¨æ€è°ƒæ•´è¡°å‡ç‡

        with torch.no_grad():
            model_params = dict(model.named_parameters())
            ema_params = dict(self.ema_model.named_parameters())

            for name, p in model_params.items():
                if p.requires_grad:
                    ema_p = ema_params[name]
                    # ema_p = d * ema_p + (1 - d) * p
                    ema_p.data.mul_(d).add_(p.data, alpha=1 - d)


# =========================
# 2) éªŒè¯å¾ªç¯
# =========================
@torch.no_grad()
def evaluate(model, dataloader, criterion, anchor_generator, precomputed_anchors, device):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè¿”å› (å¹³å‡æ€»æŸå¤±, å¹³å‡åˆ†ç±»æŸå¤±, å¹³å‡å›å½’æŸå¤±)ã€‚

    è¾“å…¥
    ----
    model : nn.Module
        å‰å‘è¿”å› (cls_preds, reg_preds) çš„æ£€æµ‹æ¨¡å‹ï¼š
        - cls_preds: [B, A, C]  (æœª sigmoid)
        - reg_preds: [B, A, 4]
    dataloader : torch.utils.data.DataLoader
        è¿­ä»£è¿”å› (imgs, targets, paths)ï¼š
        - imgs    : [B, 3, H, W]ï¼Œå·²å½’ä¸€åŒ–åˆ°ä¸è®­ç»ƒä¸€è‡´
        - targets : List[Tensor]ï¼Œé•¿åº¦ Bï¼›æ¯é¡¹ [N_i, 5]=[cls, x1, y1, x2, y2]
    criterion : nn.Module
        æŸå¤±è®¡ç®—å™¨ï¼ˆå¦‚ä½ ä¸Šé¢çš„ SSDLossï¼‰ï¼Œè°ƒç”¨æ–¹å¼ï¼š
        loss_cls, loss_reg = criterion(anchors, cls_preds, reg_preds, targets)
    anchor_generator : AnchorGenerator
        ä»…ä¸ºæ¥å£ç»Ÿä¸€ï¼Œå®é™…è¿™é‡Œä¸ç›´æ¥ç”¨ï¼ˆç”± criterion ä½¿ç”¨ï¼‰ã€‚
    precomputed_anchors : Tensor
        é¢„å…ˆç”Ÿæˆå¥½çš„æ‰€æœ‰ anchorsï¼Œå½¢çŠ¶ [A, 4]ï¼ˆä¸æ¨¡å‹è¾“å‡ºå¯¹é½ï¼‰ã€‚
    device : str | torch.device
        æ¨ç†è®¾å¤‡ã€‚

    å¤‡æ³¨
    ----
    - å†…éƒ¨ä¼šå°† model ç½®ä¸º eval æ¨¡å¼ï¼Œå¹¶åœ¨å‡½æ•°ç»“æŸåæ¢å¤ train æ¨¡å¼ï¼›
    - è®¡ç®—çš„æ˜¯ç®€å•çš„ batch å¹³å‡å†å¯¹ dataloader å–å¹³å‡ï¼ˆæ²¡æœ‰æŒ‰æ ·æœ¬æ•°åŠ æƒï¼‰ã€‚
    """
    model.eval()

    total_loss_cls = 0.0
    total_loss_reg = 0.0

    pbar = tqdm(dataloader, desc="  [Validating]ğŸŸ¡ ")
    for imgs, targets, _ in pbar:
        imgs = imgs.to(device)
        targets_on_device = [t.to(device) for t in targets]

        # å‰å‘ï¼šè¦æ±‚æ¨¡å‹è¿”å› (cls_preds, reg_preds)
        cls_preds, reg_preds = model(imgs)

        # è®¡ç®—æŸå¤±ï¼ˆanchors ç›´æ¥ä¼ å…¥ criterionï¼‰
        loss_cls, loss_reg = criterion(precomputed_anchors, cls_preds, reg_preds, targets_on_device)

        total_loss_cls += loss_cls.item()
        total_loss_reg += loss_reg.item()

        pbar.set_postfix(cls=f"{loss_cls.item():.6f}", reg=f"{loss_reg.item():.6f}")

    avg_cls_loss = total_loss_cls / len(dataloader)
    avg_reg_loss = total_loss_reg / len(dataloader)
    avg_total_loss = avg_cls_loss + avg_reg_loss

    model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
    return avg_total_loss, avg_cls_loss, avg_reg_loss
