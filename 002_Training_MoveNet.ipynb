{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77219de",
   "metadata": {},
   "source": [
    "# MoveNet_FPN è®­ç»ƒç¬”è®°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1820f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxt/miniconda3/envs/sparrow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "/home/cxt/projects/MobileSparrow/sparrow/datasets/coco_kpts.py:128: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  transforms.append(A.PadIfNeeded(\n",
      "/home/cxt/miniconda3/envs/sparrow/lib/python3.10/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/home/cxt/projects/MobileSparrow/sparrow/datasets/coco_kpts.py:141: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n",
      "  transforms.append(A.ShiftScaleRotate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ğŸŸ¢ [Training] lr: 0.000300 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16220/16220 [06:08<00:00, 44.03it/s, bg=0.00, bone=2398.89, center=18.35, hm=21.52, offsets=1024.02, regs=828.23]\n",
      "  ğŸŸ¡ [Validating] : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:04<00:00, 80.44it/s, bg=0.000000, bone=0.000606, ct=0.000554, hm=0.000223, off=0.000178, pck=2.42%, reg=0.002161, tot=0.004289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 1/100 average loss: 0.0043\n",
      "  ğŸ¯ Saved last checkpoint to ./outputs/movenet/last.pt\n",
      "  ğŸ‰ New best model found! Saved to ./outputs/movenet/best.pt\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ğŸŸ¢ [Training] lr: 0.000150 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16220/16220 [06:13<00:00, 43.44it/s, bg=0.00, bone=9.95, center=8.79, hm=3.60, offsets=2.91, regs=36.90]\n",
      "  ğŸŸ¡ [Validating] : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:04<00:00, 68.02it/s, bg=0.000000, bone=0.000595, ct=0.000538, hm=0.000215, off=0.000121, pck=2.34%, reg=0.002162, tot=0.004206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 2/100 average loss: 0.0042\n",
      "  ğŸ¯ Saved last checkpoint to ./outputs/movenet/last.pt\n",
      "  ğŸ‰ New best model found! Saved to ./outputs/movenet/best.pt\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ğŸŸ¢ [Training] lr: 0.000300 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16220/16220 [06:08<00:00, 44.02it/s, bg=0.00, bone=9.86, center=8.63, hm=3.53, offsets=3.16, regs=37.29]\n",
      "  ğŸŸ¡ [Validating] : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:05<00:00, 63.65it/s, bg=0.000000, bone=0.000592, ct=0.000530, hm=0.000213, off=0.000102, pck=1.78%, reg=0.002146, tot=0.004152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 3/100 average loss: 0.0042\n",
      "  ğŸ¯ Saved last checkpoint to ./outputs/movenet/last.pt\n",
      "  ğŸ‰ New best model found! Saved to ./outputs/movenet/best.pt\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ğŸŸ¢ [Training] lr: 0.000300 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16220/16220 [06:16<00:00, 43.07it/s, bg=0.00, bone=9.81, center=8.53, hm=3.50, offsets=2.96, regs=37.13]\n",
      "  ğŸŸ¡ [Validating] : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:05<00:00, 64.18it/s, bg=0.000000, bone=0.000591, ct=0.000527, hm=0.000211, off=0.000095, pck=2.15%, reg=0.002138, tot=0.004128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 4/100 average loss: 0.0041\n",
      "  ğŸ¯ Saved last checkpoint to ./outputs/movenet/last.pt\n",
      "  ğŸ‰ New best model found! Saved to ./outputs/movenet/best.pt\n",
      "\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ğŸŸ¢ [Training] lr: 0.000299 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16220/16220 [06:23<00:00, 42.32it/s, bg=0.00, bone=9.79, center=8.48, hm=3.48, offsets=2.83, regs=36.98]\n",
      "  ğŸŸ¡ [Validating] : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:05<00:00, 59.16it/s, bg=0.000000, bone=0.000590, ct=0.000523, hm=0.000210, off=0.000091, pck=2.09%, reg=0.002134, tot=0.004113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 5/100 average loss: 0.0041\n",
      "  ğŸ¯ Saved last checkpoint to ./outputs/movenet/last.pt\n",
      "  ğŸ‰ New best model found! Saved to ./outputs/movenet/best.pt\n",
      "  ğŸ“Š Visualized predictions on test image\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "visualize_movenet() got an unexpected keyword argument 'draw_bbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 267\u001b[0m\n\u001b[1;32m    264\u001b[0m         img_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img_bgr, (\u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m600\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR)\n\u001b[1;32m    265\u001b[0m         save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(viz_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m         \u001b[43mvisualize_movenet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# â˜… ç”¨ EMA æ¨¡å‹\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_resized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_movenet_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_STRIDE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtopk_centers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcenter_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeypoint_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraw_on_orig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraw_heatmaps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# å¦‚æœä½ çš„ visualize_movenet ä»æ”¯æŒ bbox å¼€å…³ï¼š\u001b[39;49;00m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# åªç”»å…³é”®ç‚¹+éª¨æ¶\u001b[39;49;00m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraw_bbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraw_skeleton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# è‹¥é›†æˆäº†å•äººç­›é€‰ï¼ˆforce_singleï¼‰ï¼Œå¯æŒ‰éœ€åŠ ï¼š\u001b[39;49;00m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# force_single=True\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training Finished ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparrow/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: visualize_movenet() got an unexpected keyword argument 'draw_bbox'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # MoveNet_FPN è®­ç»ƒç¬”è®°\n",
    "\n",
    "# %% [markdown]\n",
    "# ## å¯¼å…¥å·¥ç¨‹\n",
    "\n",
    "# %%\n",
    "# å¯¼å…¥ç³»ç»Ÿåº“\n",
    "import os\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å¯¼å…¥sparrow\n",
    "from sparrow.models.movenet_fpn import MoveNet_FPN, decode_movenet_outputs\n",
    "from sparrow.datasets.coco_kpts import create_kpts_dataloader\n",
    "from sparrow.losses.movenet_loss import MoveNetLoss, evaluate_local\n",
    "from sparrow.utils.ema import EMA\n",
    "from sparrow.utils.visual_movenet import visualize_movenet\n",
    "\n",
    "# å¯¼å…¥torchåº“\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# %% [markdown]\n",
    "# ## å‚æ•°è®¾ç½®\n",
    "#\n",
    "# ### ç³»ç»Ÿå‚æ•°\n",
    "\n",
    "# %%\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "INPUT_SIZE = 192\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "NUM_JOINTS = 17\n",
    "UPSAMPLE = True\n",
    "TARGET_STRIDE = 4\n",
    "\n",
    "COCO_ROOT = \"./data/coco2017_movenet\"            # COCOè®­ç»ƒæ•°æ®é›†\n",
    "WEIGHTS_DIR = \"./outputs/movenet\"                # ä¿å­˜æƒé‡çš„ç›®å½•\n",
    "TEST_IMAGE_PATH = \"./res/girl_with_bags.png\"     # æµ‹è¯•å›¾ç‰‡è·¯å¾„\n",
    "\n",
    "# %% [markdown]\n",
    "# ### å­¦ä¹ å‚æ•°\n",
    "\n",
    "# %%\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 100\n",
    "BEST_VAL_LOSS = float('inf')\n",
    "\n",
    "WARMUP_EPOCHS = 2               # çº¿æ€§é¢„çƒ­ epoch æ•°\n",
    "GRADIENT_CLIP_VAL = 5.0         # æ¢¯åº¦è£å‰ªé˜ˆå€¼\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "\n",
    "# %% [markdown]\n",
    "# ## åˆ›å»ºæ¨¡å‹\n",
    "\n",
    "# %%\n",
    "backbone_fpn = timm.create_model(\n",
    "    'mobilenetv3_large_100', pretrained=True, features_only=True, out_indices=(2, 3, 4)\n",
    ")\n",
    "model_fpn = MoveNet_FPN(\n",
    "    backbone_fpn,\n",
    "    num_joints=NUM_JOINTS,\n",
    "    fpn_out_channels=128,\n",
    "    upsample_to_quarter=UPSAMPLE,\n",
    "    out_stride=TARGET_STRIDE\n",
    ").to(device)\n",
    "\n",
    "# EMA è¯„ä¼°å™¨\n",
    "ema = EMA(model_fpn)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## åŠ è½½æ•°æ®\n",
    "\n",
    "# %%\n",
    "# è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆå»ºè®®æ­¤å¤„æŒ‰éœ€å¼€å¯è½»åº¦å¢å¼ºï¼‰\n",
    "train_aug_config = {\"use_flip\": False, \"use_color_aug\": False}\n",
    "train_loader = create_kpts_dataloader(\n",
    "    dataset_root=COCO_ROOT,\n",
    "    img_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_stride=TARGET_STRIDE,\n",
    "    pin_memory=True,\n",
    "    aug_cfg=train_aug_config,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# éªŒè¯ï¼šä¸¥æ ¼å…³é—­éšæœºå¢å¼ºï¼Œç¡®ä¿æ›²çº¿ç¨³å®š\n",
    "test_aug_config = {\"use_flip\": False, \"use_color_aug\": False}\n",
    "val_loader = create_kpts_dataloader(\n",
    "    dataset_root=COCO_ROOT,\n",
    "    img_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    target_stride=TARGET_STRIDE,\n",
    "    pin_memory=True,\n",
    "    aug_cfg=test_aug_config,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## æŸå¤±/ä¼˜åŒ–/è°ƒåº¦\n",
    "\n",
    "# %%\n",
    "# â˜… æŸå¤±å‡½æ•°ï¼šå¼€å¯éª¨æ¶ä¸€è‡´æ€§ï¼ˆbg å…ˆå…³ï¼‰\n",
    "criterion = MoveNetLoss(\n",
    "    hm_weight=1.0, ct_weight=1.0, reg_weight=1.5, off_weight=1.0,\n",
    "    bone_weight=0.15,   # å»ºè®® 0.10~0.20 ä¹‹é—´å¾®è°ƒ\n",
    "    bg_weight=0.0\n",
    ")\n",
    "\n",
    "# â˜… ä¼˜åŒ–å™¨ï¼šæ¨è AdamW æ›´ç¨³ï¼ˆå¦‚éœ€ç”¨ Adamï¼ŒæŠŠä¸‹é¢ä¸€è¡Œæ”¹å› torch.optim.Adamï¼‰\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_fpn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## åŠ è½½é¢„è®­ç»ƒ/æ–­ç‚¹\n",
    "\n",
    "# %%\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "last_pt_path = os.path.join(WEIGHTS_DIR, \"last.pt\")\n",
    "if os.path.exists(last_pt_path):\n",
    "    print(\"--- Resuming training from last.pt ---\")\n",
    "    checkpoint = torch.load(last_pt_path, map_location=device)\n",
    "\n",
    "    model_fpn.load_state_dict(checkpoint['model'])\n",
    "    ema.ema_model.load_state_dict(checkpoint['ema_model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    START_EPOCH = checkpoint['epoch'] + 1\n",
    "    BEST_VAL_LOSS = checkpoint['best_val_loss']\n",
    "\n",
    "    print(f\"Resumed from epoch {START_EPOCH-1}. Best validation loss so far: {BEST_VAL_LOSS:.4f}\")\n",
    "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## è®­ç»ƒå¾ªç¯\n",
    "\n",
    "# %%\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "\n",
    "# é¢„çƒ­æ­¥æ•°\n",
    "warmup_steps = WARMUP_EPOCHS * len(train_loader)\n",
    "current_step = START_EPOCH * len(train_loader)\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    model_fpn.train()\n",
    "\n",
    "    # ç»Ÿè®¡é¡¹\n",
    "    epoch_loss_heatmap = 0.0\n",
    "    epoch_loss_center  = 0.0\n",
    "    epoch_loss_regs    = 0.0\n",
    "    epoch_loss_offsets = 0.0\n",
    "    epoch_loss_bone    = 0.0\n",
    "    epoch_loss_bg      = 0.0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    pbar = tqdm(train_loader, desc=f\"  ğŸŸ¢ [Training] lr: {optimizer.param_groups[0]['lr']:.6f} \")\n",
    "\n",
    "    for i, (imgs, labels, kps_masks, _) in enumerate(pbar):\n",
    "        # çº¿æ€§é¢„çƒ­\n",
    "        if current_step < warmup_steps:\n",
    "            lr_scale = (current_step + 1) / max(1, warmup_steps)\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = LEARNING_RATE * lr_scale\n",
    "\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        kps_masks = kps_masks.to(device, non_blocking=True)\n",
    "\n",
    "        # --- æŠŠ [B,17] æˆ– [B,17,1,1] å¹¿æ’­ä¸º [B,17,Hf,Wf] ---\n",
    "        Hf, Wf = labels.shape[-2], labels.shape[-1]\n",
    "        if kps_masks.dim() == 2 and kps_masks.shape[1] == 17:\n",
    "            kps_masks = kps_masks[:, :, None, None].float().expand(-1, -1, Hf, Wf).contiguous()\n",
    "        elif kps_masks.dim() == 4 and kps_masks.shape[2] == 1 and kps_masks.shape[3] == 1:\n",
    "            kps_masks = kps_masks.float().expand(-1, -1, Hf, Wf).contiguous()\n",
    "        # å¦åˆ™åº”å·²æ˜¯ [B,17,Hf,Wf]ï¼ŒæŒ‰æ–°ç‰ˆæœ¬æ­£å¸¸ä½¿ç”¨\n",
    "\n",
    "        # å‰å‘\n",
    "        preds = model_fpn(imgs)\n",
    "        total_loss, loss_dict = criterion(preds, labels, kps_masks)\n",
    "\n",
    "        # åä¼ \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        torch.nn.utils.clip_grad_norm_(model_fpn.parameters(), max_norm=GRADIENT_CLIP_VAL)\n",
    "\n",
    "        # æ›´æ–°\n",
    "        optimizer.step()\n",
    "        ema.update(model_fpn)\n",
    "        current_step += 1\n",
    "\n",
    "        # ç´¯è®¡æ˜¾ç¤º\n",
    "        epoch_loss_heatmap += float(loss_dict[\"loss_heatmap\"])\n",
    "        epoch_loss_center  += float(loss_dict[\"loss_center\"])\n",
    "        epoch_loss_regs    += float(loss_dict[\"loss_regs\"])\n",
    "        epoch_loss_offsets += float(loss_dict[\"loss_offsets\"])\n",
    "        if criterion.bone_weight > 0:\n",
    "            epoch_loss_bone  += float(loss_dict[\"loss_bone\"])\n",
    "        if criterion.bg_weight > 0:\n",
    "            epoch_loss_bg    += float(loss_dict[\"loss_bg\"])\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            hm=f\"{epoch_loss_heatmap:.2f}\",\n",
    "            center=f\"{epoch_loss_center:.2f}\",\n",
    "            regs=f\"{epoch_loss_regs:.2f}\",\n",
    "            offsets=f\"{epoch_loss_offsets:.2f}\",\n",
    "            bone=(f\"{epoch_loss_bone:.2f}\" if criterion.bone_weight > 0 else \"0.00\"),\n",
    "            bg=(f\"{epoch_loss_bg:.2f}\" if criterion.bg_weight > 0 else \"0.00\"),\n",
    "        )\n",
    "\n",
    "    # è°ƒåº¦å™¨æ­¥è¿›ï¼ˆæ”¾åœ¨ä¸€ä¸ª epoch ç»“æŸåï¼‰\n",
    "    if epoch >= WARMUP_EPOCHS - 1:\n",
    "        scheduler.step()\n",
    "\n",
    "    # ===== éªŒè¯ï¼ˆç”¨ EMA æ¨¡å‹ï¼‰=====\n",
    "    avg_total_loss, avg_dict = evaluate_local(\n",
    "        ema.ema_model, val_loader, criterion, device,\n",
    "        decoder=decode_movenet_outputs, stride=TARGET_STRIDE\n",
    "    )\n",
    "    print(f\"  ğŸ“œ Epoch {epoch+1}/{EPOCHS} average loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "    # ===== ä¿å­˜æƒé‡ =====\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': model_fpn.state_dict(),\n",
    "        'ema_model': ema.ema_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'best_val_loss': BEST_VAL_LOSS,\n",
    "    }\n",
    "    torch.save(checkpoint, last_pt_path)\n",
    "    print(f\"  ğŸ¯ Saved last checkpoint to {last_pt_path}\")\n",
    "\n",
    "    if avg_total_loss < BEST_VAL_LOSS:\n",
    "        BEST_VAL_LOSS = avg_total_loss\n",
    "        checkpoint['best_val_loss'] = BEST_VAL_LOSS\n",
    "        best_pt_path = os.path.join(WEIGHTS_DIR, \"best.pt\")\n",
    "        torch.save(checkpoint, best_pt_path)\n",
    "        print(f\"  ğŸ‰ New best model found! Saved to {best_pt_path}\")\n",
    "\n",
    "    # ===== æ¯ 5 ä¸ª epoch å¯è§†åŒ–ï¼ˆç”¨ EMA æ¨¡å‹ï¼›åªç”»å…³é”®ç‚¹+éª¨æ¶ï¼‰=====\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  ğŸ“Š Visualized predictions on test image\")\n",
    "        viz_dir = os.path.join(WEIGHTS_DIR, \"viz\")\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "        import cv2\n",
    "        img_bgr = cv2.imread(TEST_IMAGE_PATH)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"TEST_IMAGE_PATH not found: {TEST_IMAGE_PATH}\")\n",
    "\n",
    "        img_resized = cv2.resize(img_bgr, (600, 600), interpolation=cv2.INTER_LINEAR)\n",
    "        save_path = os.path.join(viz_dir, f\"epoch_{epoch+1:03d}.png\")\n",
    "\n",
    "        visualize_movenet(\n",
    "            model=ema.ema_model,        # â˜… ç”¨ EMA æ¨¡å‹\n",
    "            image=img_resized,\n",
    "            device=device,\n",
    "            decoder=decode_movenet_outputs,\n",
    "            input_size=INPUT_SIZE,\n",
    "            stride=TARGET_STRIDE,\n",
    "            topk_centers=3,\n",
    "            center_thresh=0.10,\n",
    "            keypoint_thresh=0.03,\n",
    "            draw_on_orig=True,\n",
    "            draw_heatmaps=True,\n",
    "            save_path=save_path,\n",
    "            show=False,\n",
    "            # å¦‚æœä½ çš„ visualize_movenet ä»æ”¯æŒ bbox å¼€å…³ï¼š\n",
    "            # åªç”»å…³é”®ç‚¹+éª¨æ¶\n",
    "            draw_bbox=False,\n",
    "            draw_skeleton=True,\n",
    "            # è‹¥é›†æˆäº†å•äººç­›é€‰ï¼ˆforce_singleï¼‰ï¼Œå¯æŒ‰éœ€åŠ ï¼š\n",
    "            # force_single=True\n",
    "        )\n",
    "\n",
    "print(\"--- Training Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
