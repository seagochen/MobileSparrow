{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2ddc9e",
   "metadata": {},
   "source": [
    "# SSDLite_FPN è®­ç»ƒç¬”è®°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899aa05f",
   "metadata": {},
   "source": [
    "## å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96b1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torchvision\n",
    "!pip -q install tqdm\n",
    "!pip -q install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39a437",
   "metadata": {},
   "source": [
    "## å¯¼å…¥å·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bb3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxt/miniconda3/envs/sparrow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥ç³»ç»Ÿåº“\n",
    "import os\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# å¯¼å…¥sparrow\n",
    "from sparrow.models.ssdlite_fpn import SSDLite_FPN\n",
    "from sparrow.datasets.coco_dets import create_dets_dataloader\n",
    "from sparrow.losses.ssdlite_loss import SSDLoss, AnchorGenerator\n",
    "from sparrow.utils.torch_utils import EMA, evaluate\n",
    "from sparrow.utils.visualization import visualize_predictions\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# å¯¼å…¥torchåº“\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cc0dd",
   "metadata": {},
   "source": [
    "## å‚æ•°è®¾ç½®\n",
    "\n",
    "### ç³»ç»Ÿå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcf6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "INPUT_SIZE = 320\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 80 # COCO æ•°æ®é›†å®šä¹‰æ•°\n",
    "ANCHOR_SIZES = [32, 64, 128, 256, 512]\n",
    "ANCHOR_RATIOS = [0.5, 1.0, 2.0, 1/3.0, 3.0]\n",
    "\n",
    "COCO_ROOT = \"./data/coco2017_ssdlite\"\n",
    "WEIGHTS_DIR = \"./outputs/ssdlite\" # ä¿å­˜æƒé‡çš„ç›®å½•\n",
    "TEST_IMAGE_PATH = \"./res/india_road.png\" # ä½ çš„æµ‹è¯•å›¾ç‰‡è·¯å¾„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034db9c",
   "metadata": {},
   "source": [
    "### å­¦ä¹ å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f45e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPOCH = 0\n",
    "LOG_INTERVAL_SAMPLES = 1000\n",
    "EPOCHS=100\n",
    "log_interval_batches = max(1, LOG_INTERVAL_SAMPLES // BATCH_SIZE)\n",
    "BEST_VAL_LOSS = float('inf')\n",
    "LEARNING_RATE = 1e-4 # åˆå§‹å­¦ä¹ ç‡\n",
    "WEIGHT_DECAY = 1e-3\n",
    "WARMUP_EPOCHS = 2 # <--- æ–°å¢ï¼šé¢„çƒ­çš„ epoch æ•°é‡\n",
    "GRADIENT_CLIP_VAL = 5.0 # <--- æ–°å¢ï¼šæ¢¯åº¦è£å‰ªçš„é˜ˆå€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae9896",
   "metadata": {},
   "source": [
    "## åˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da76bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (classifier.bias, classifier.weight, conv_head.bias, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "backbone_fpn = timm.create_model('mobilenetv3_large_100', pretrained=True, features_only=True, out_indices=(2, 3, 4))\n",
    "model_fpn = SSDLite_FPN(backbone_fpn, num_classes=NUM_CLASSES, fpn_out_channels=128, num_anchors=len(ANCHOR_RATIOS))\n",
    "model_fpn.to(device)\n",
    "\n",
    "# EMAè¯„ä¼°å™¨\n",
    "ema = EMA(model_fpn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054edf0",
   "metadata": {},
   "source": [
    "## åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f7ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxt/projects/MobileSparrow/sparrow/datasets/coco_dets.py:182: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  transforms.append(A.PadIfNeeded(\n",
      "/home/cxt/miniconda3/envs/sparrow/lib/python3.10/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/home/cxt/projects/MobileSparrow/sparrow/datasets/coco_dets.py:194: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n",
      "  transforms.append(A.ShiftScaleRotate(\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ (æ¥è‡ª dataloader.py)\n",
    "train_aug_config = { \"rotate_deg\": 15.0, \"min_box_size\": 2.0 }\n",
    "train_loader = create_dets_dataloader(\n",
    "    dataset_root=COCO_ROOT,\n",
    "    img_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    aug_cfg=train_aug_config,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "# åˆ›å»ºéªŒè¯é›†æ•°æ®åŠ è½½å™¨\n",
    "val_aug_config = { \"min_box_size\": 2.0 } # éªŒè¯é›†é€šå¸¸ä¸åšå¤æ‚å¢å¼º\n",
    "val_loader = create_dets_dataloader(\n",
    "    dataset_root=COCO_ROOT,\n",
    "    img_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE * 2,  # éªŒè¯æ—¶é€šå¸¸å¯ä»¥ç”¨æ›´å¤§çš„ batch size\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    aug_cfg=val_aug_config,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6535f",
   "metadata": {},
   "source": [
    "## æŸå¤±ä¼˜åŒ–è°ƒåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbfa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸå¤±å‡½æ•°\n",
    "criterion = SSDLoss(num_classes=NUM_CLASSES)\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(model_fpn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# å­¦ä¹ è°ƒåº¦å™¨\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc13295",
   "metadata": {},
   "source": [
    "## åŠ è½½é¢„è®­ç»ƒæƒé‡\n",
    "\n",
    "ç¡®ä¿æ¯æ¬¡è®­ç»ƒä¸å¿…ä»å¤´å†æ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0086d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resuming training from last.pt ---\n",
      "Resumed from epoch 7. Best validation loss so far: 0.5042\n",
      "Current learning rate is 0.000099\n"
     ]
    }
   ],
   "source": [
    "# ç¡®ä¿å­˜æ”¾é¢„è®­ç»ƒçš„ç›®å½•å­˜åœ¨\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True) # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "\n",
    "# æ–­ç‚¹ç»­è®­é€»è¾‘\n",
    "last_pt_path = os.path.join(WEIGHTS_DIR, \"last.pt\")\n",
    "if os.path.exists(last_pt_path):\n",
    "    print(\"--- Resuming training from last.pt ---\")\n",
    "\n",
    "    # åŠ è½½ptæ–‡ä»¶\n",
    "    checkpoint = torch.load(last_pt_path, map_location=device)\n",
    "    \n",
    "    # ä»ptä¸­è¯»å–æ¨¡å‹æƒé‡\n",
    "    model_fpn.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    # åŠ è½½EMAçŠ¶æ€\n",
    "    ema.ema_model.load_state_dict(checkpoint['ema_model'])\n",
    "\n",
    "    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    # æ›´æ–°EPOCHçŠ¶æ€\n",
    "    START_EPOCH = checkpoint['epoch'] + 1\n",
    "    \n",
    "    # æ›´æ–°æœ€ä½³æŸå¤±çŠ¶æ€\n",
    "    BEST_VAL_LOSS = checkpoint['best_val_loss']\n",
    "    \n",
    "    # æ‰“å°ç¡®è®¤æ¶ˆæ¯\n",
    "    print(f\"Resumed from epoch {START_EPOCH-1}. Best validation loss so far: {BEST_VAL_LOSS:.4f}\")\n",
    "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36111e8a",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç† anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92c45f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing anchors for fixed input size...\n",
      "Anchors pre-computed. Shape: torch.Size([10670, 4])\n"
     ]
    }
   ],
   "source": [
    "# --- é¢„è®¡ç®—é”šæ¡† (æ ¸å¿ƒæ­¥éª¤) ---\n",
    "print(\"Pre-computing anchors for fixed input size...\")\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=ANCHOR_SIZES,\n",
    "    aspect_ratios=ANCHOR_RATIOS\n",
    ")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè¾“å…¥\n",
    "dummy_input = torch.randn(1, 3, INPUT_SIZE, INPUT_SIZE).to(device)\n",
    "\n",
    "# è®¾ç½®ä¸º eval æ¨¡å¼ï¼Œå¹¶ç¡®ä¿æ²¡æœ‰æ¢¯åº¦è®¡ç®—\n",
    "model_fpn.eval()\n",
    "with torch.no_grad():\n",
    "    # æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡ç‰¹å¾æå–æµç¨‹ï¼Œä»¥è·å–ç‰¹å¾å›¾å°ºå¯¸\n",
    "    features = model_fpn.backbone(dummy_input)\n",
    "    p3, p4, p5 = model_fpn.fpn(features)\n",
    "    p6 = model_fpn.extra_layers[0](p5)\n",
    "    p7 = model_fpn.extra_layers[1](p6)\n",
    "    feature_maps_for_size_calc = [p3, p4, p5, p6, p7]\n",
    "\n",
    "# ä½¿ç”¨è·å–çš„ç‰¹å¾å›¾åˆ—è¡¨ç”Ÿæˆä¸€æ¬¡æ€§çš„ã€å®Œæ•´çš„é”šæ¡†ç½‘æ ¼\n",
    "# è¿™ä¸ª precomputed_anchors å°†åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¢«é‡å¤ä½¿ç”¨\n",
    "precomputed_anchors = anchor_generator.generate_anchors_on_grid(feature_maps_for_size_calc, device)\n",
    "print(f\"Anchors pre-computed. Shape: {precomputed_anchors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f57db9",
   "metadata": {},
   "source": [
    "## å¯åŠ¨è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f16739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Logging average loss every 125 batches.\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Training]ğŸŸ¢ lr: 0.000099 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14658/14658 [07:19<00:00, 33.34it/s, cls=0.327699, reg=156.496334]\n",
      "  [Validating]ğŸŸ¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:06<00:00, 45.87it/s, cls=2.247292, reg=62.440883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 9/100 average loss: 0.2087\n",
      "\n",
      "  Saved last checkpoint to ./outputs/ssdlite/last.pt\n",
      "  >>> New best model found! Saved to ./outputs/ssdlite/best.pt\n",
      "\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Training]ğŸŸ¢ lr: 0.000098 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14658/14658 [07:18<00:00, 33.39it/s, cls=0.324048, reg=154.241635]\n",
      "  [Validating]ğŸŸ¡ : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:06<00:00, 48.58it/s, cls=0.994695, reg=90.577492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“œ Epoch 10/100 average loss: 0.2954\n",
      "\n",
      "  Saved last checkpoint to ./outputs/ssdlite/last.pt\n",
      "\n",
      "--- Visualizing predictions at epoch 10 ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "visualize_predictions() got an unexpected keyword argument 'anchor_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# 2) å¯è§†åŒ–ä¿å­˜å æ¡†ç»“æœï¼ˆä¼šå›åˆ°è¿™å¼  800x600 çš„åæ ‡ç³»ä¸Šç»˜åˆ¶ï¼‰\u001b[39;00m\n\u001b[1;32m    105\u001b[0m         save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(viz_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresized_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# ç”¨ç¼©æ”¾åçš„å›¾ç‰‡\u001b[39;49;00m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43manchor_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ä»…ç”¨äºåæ ‡è½¬æ¢\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprecomputed_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecomputed_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconf_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnms_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# ä¿å­˜ï¼Œä¸ show\u001b[39;49;00m\n\u001b[1;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdraw_on_original\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# åœ¨åŸå›¾åæ ‡(æ­¤æ—¶æ˜¯800x600)ä¸Šç”»\u001b[39;49;00m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training Finished ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sparrow/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: visualize_predictions() got an unexpected keyword argument 'anchor_generator'"
     ]
    }
   ],
   "source": [
    "# --- è®­ç»ƒå¾ªç¯ ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "model_fpn.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼\n",
    "print(f\"Logging average loss every {log_interval_batches} batches.\")  # æ—¥å¿—çš„æ‰“å°é¢‘ç‡ \n",
    "\n",
    "# è®¡ç®—é¢„çƒ­çš„æ€»æ­¥æ•°\n",
    "warmup_steps = WARMUP_EPOCHS * len(train_loader)\n",
    "current_step = START_EPOCH * len(train_loader)\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    model_fpn.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    \n",
    "    epoch_loss_cls = 0.0\n",
    "    epoch_loss_reg = 0.0\n",
    "\n",
    "    # è¿›åº¦æ¡ä¿¡æ¯\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    pbar = tqdm(train_loader, desc=f\"  ğŸŸ¢ [Training] lr: {optimizer.param_groups[0]['lr']:.6f} \")\n",
    "    \n",
    "    for i, (imgs, targets, _) in enumerate(pbar):\n",
    "        # --- å­¦ä¹ ç‡é¢„çƒ­é€»è¾‘ ---\n",
    "        if current_step < warmup_steps:\n",
    "            # çº¿æ€§é¢„çƒ­\n",
    "            lr_scale = (current_step + 1) / warmup_steps\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = LEARNING_RATE * lr_scale\n",
    "\n",
    "        # --- æ­£å¸¸è®­ç»ƒæ­¥éª¤ ---\n",
    "        imgs = imgs.to(device)\n",
    "        targets_on_device = [t.to(device) for t in targets]\n",
    "        cls_preds, reg_preds = model_fpn(imgs)\n",
    "        loss_cls, loss_reg = criterion(precomputed_anchors, cls_preds, reg_preds, targets_on_device)\n",
    "        total_loss = loss_cls + loss_reg\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # --- æ–°å¢ï¼šæ¢¯åº¦è£å‰ª ---\n",
    "        torch.nn.utils.clip_grad_norm_(model_fpn.parameters(), max_norm=GRADIENT_CLIP_VAL)\n",
    "        \n",
    "        # å¾®è°ƒæ¨¡å‹å‚æ•°\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- æ›´æ–° EMA ---\n",
    "        # æ›´æ–° EMA å’Œæ­¥æ•°è®¡æ•°å™¨\n",
    "        ema.update(model_fpn)\n",
    "        current_step += 1\n",
    "        \n",
    "        epoch_loss_cls += loss_cls.item()\n",
    "        epoch_loss_reg += loss_reg.item()\n",
    "\n",
    "        # æ˜¾ç¤ºå½“å‰è®­ç»ƒä¿¡æ¯\n",
    "        pbar.set_postfix(cls=f\"{epoch_loss_cls:.6f}\", reg=f\"{epoch_loss_reg:.6f}\")\n",
    "    # end-for: è®­ç»ƒç»“æŸ\n",
    "\n",
    "    # æ¯ä¸ª epoch ç»“æŸåï¼Œæ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    if epoch >= WARMUP_EPOCHS - 1: # -1 æ˜¯å› ä¸º step() åº”åœ¨ optimizer.step() ä¹‹åè°ƒç”¨\n",
    "        scheduler.step()\n",
    "\n",
    "    # æ¯ä¸ª epoch ç»“æŸåï¼Œè¿›è¡ŒéªŒè¯\n",
    "    avg_total_loss, _, _ = evaluate(ema.ema_model, val_loader, criterion, anchor_generator, precomputed_anchors, device)\n",
    "\n",
    "    # ç”Ÿæˆæœ¬æ¬¡epochæŠ¥å‘Š\n",
    "    print(f\"  ğŸ“œ Epoch {epoch+1}/{EPOCHS} average loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "    # ä¿å­˜ last.pt å’Œ best.pt\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': model_fpn.state_dict(),\n",
    "        'ema_model': ema.ema_model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'best_val_loss': BEST_VAL_LOSS,\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜ last.pt\n",
    "    torch.save(checkpoint, last_pt_path)\n",
    "    print(f\"  ğŸ¯ Saved last checkpoint to {last_pt_path}\")\n",
    "    \n",
    "    # å¦‚æœå½“å‰æ˜¯æœ€ä½³æ¨¡å‹ï¼Œåˆ™ä¿å­˜ best.pt\n",
    "    if avg_total_loss < BEST_VAL_LOSS:\n",
    "        BEST_VAL_LOSS = avg_total_loss\n",
    "        checkpoint['best_val_loss'] = BEST_VAL_LOSS # æ›´æ–° checkpoint ä¸­çš„æœ€ä½³æŸå¤±\n",
    "        best_pt_path = os.path.join(WEIGHTS_DIR, \"best.pt\")\n",
    "        torch.save(checkpoint, best_pt_path)\n",
    "        print(f\"  ğŸ‰ New best model found! Saved to {best_pt_path}\")\n",
    "        \n",
    "    # --- æ¯ 10 ä¸ª epochï¼Œå¯è§†åŒ–ä¸€æ¬¡é¢„æµ‹ç»“æœ ---\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  ğŸ“Š Visualized predictions on test image\")\n",
    "        viz_dir = os.path.join(WEIGHTS_DIR, \"viz\")\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "        # 1) å…ˆæŠŠæµ‹è¯•å›¾ç‰‡ç¼©æ”¾åˆ° 800x600ï¼Œå¹¶å¦å­˜ä¸ºä¸´æ—¶æ–‡ä»¶\n",
    "        import cv2\n",
    "        img_bgr = cv2.imread(TEST_IMAGE_PATH)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"TEST_IMAGE_PATH not found: {TEST_IMAGE_PATH}\")\n",
    "        img_resized = cv2.resize(img_bgr, (800, 600), interpolation=cv2.INTER_LINEAR)  # éç­‰æ¯”ä¾‹ç›´æ¥æ‹‰ä¼¸\n",
    "        # å¦‚æœä½ æƒ³â€œç­‰æ¯”ä¾‹ç¼©æ”¾ + ç°è¾¹å¡«å…… 800x600â€ï¼Œæˆ‘åœ¨ä¸‹æ–¹ç»™äº†å¯é€‰å‡½æ•°\n",
    "\n",
    "        resized_path = os.path.join(viz_dir, f\"epoch_{epoch+1:03d}_resized_800x600.png\")\n",
    "        cv2.imwrite(resized_path, img_resized)\n",
    "\n",
    "        # 2) å¯è§†åŒ–ä¿å­˜å æ¡†ç»“æœï¼ˆä¼šå›åˆ°è¿™å¼  800x600 çš„åæ ‡ç³»ä¸Šç»˜åˆ¶ï¼‰\n",
    "        save_path = os.path.join(viz_dir, f\"epoch_{epoch+1:03d}.png\")\n",
    "        visualize_predictions(\n",
    "            model=ema.ema_model,\n",
    "            image_path=resized_path,            # ç”¨ç¼©æ”¾åçš„å›¾ç‰‡\n",
    "            device=device,\n",
    "            precomputed_anchors=precomputed_anchors,\n",
    "            conf_thresh=0.3,\n",
    "            nms_thresh=0.45,\n",
    "            save_path=save_path,                # ä¿å­˜ï¼Œä¸ show\n",
    "            draw_on_orig=True,                  # æ³¨æ„ï¼šæ–°å‚æ•°å\n",
    "            show=False\n",
    "        )\n",
    "            \n",
    "    # æ¢è¡Œ\n",
    "    print()\n",
    "\n",
    "print(\"--- Training Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
